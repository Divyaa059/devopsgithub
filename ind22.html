LiNUX
ques.Create the following users,groups and group membership
-A group named sysadm
-A user "harry" who belongs to sysadm as a secondary group.
-A user "natasha" who belongs to sysadm as a secondary group.
-A user "sarah" who does not have the access to an interactive shell and who is not a member of sysadm group.
-"harry" "natasha"  and "sarah" should all have password of password 
SOl:-
cat /etc/group
groupadd sysadm
cat /etc/group/ -i sysadm
useradd harry
passwd harry same for natasha and sarah
usermod -G sysadm harry
usermod -G sysadm natasha
usermod -s /sbin/nologin sarah
cat /etc/passwd | grep -i sarah
####################################################################################################################
ques.create a collabrative directory /shared/sysadm with th following characteristics:
-Group owernship of /shared/sysadm is sysadm
-The directory should be readable , writable and accesiable to member of sysadm . but not to any other user.
-Files created in /shared/sysadm automatically have group owernshipset to the sysadm group
sol:-
mkdir -p /shared/sysadm
ll -d /shared/sysadm
chgrp sysadm /shared/sysadm
ll -d /shared/syadm
chmod 770 /shared/sysadm
ll -d /shared/syadm
chmod g+s /shared/sysadm
su -harry
cd /shared/sysadm
ll touch harry-file
##########################################################################################################################
 useradd ram
    2  cat /etc/passwd
    3  passwd ram
    4  groupadd sales
    5  groupadd marketing
    6  ll
    7  cat /etc/passwd
    8  usermod -a -G sales marketing ram
    9  cat /etc/passwd
   10  usermod -a -G sales,marketing ram
   11  cat /etc/passwd
   12  cat /etc/group
   13  history
##########################################################################################################################
   useradd sanjay
   passwd sanjay
   vim /etc/sudoers
whereis useradd 
whereis passwd
   user1 = add ....useradd user2 , .....passwd user2
   sri     ALL=(ALL)       /usr/sbin/useradd pooja
   
   sudo su sanjay

   useradd pooja
   passwd pooja

##########################################################################################################################
TOMCAT-SERVER
* first 3 instances dev-server , jenkins-server , web-server , tomcat-server
dev-server = t2.micro , size = 12
jenkins-server = t2.medium , size = 8
tomcat-server = t2,micro , size = 10
ON DEV-SERVER 
sudo su -
set hostname - dev-server.example.com
bash
yum install git 
CREATE A PUBLIC REPO AND CLONE
 1  yum install git
    2  history
    3  git clone git@github.com:sanjayguruji/web-repo.git
    4  git clone https://github.com/sanjayguruji/web-repo.git
    5  ssh-keygen
    6  cd .ssh/
    7  cat id_rsa.pub (add the ssh key to the github)
    8  cd
    9  cd web-repo
   10  git init
   11  git add .
   12  git commit -m devops -a
   13  git branch -M main
   14  git remote add origin git@github.com:mansi-lti/tomcat.git
   15  git remote remove origin
   16  git remote add origin git@github.com:mansi-lti/tomcat.git
   17  git push origin main
CONNECT TO JENKINS SERVER
sudo su -
hostname
bash
 wget -O /etc/yum.repos.d/jenkins.repo     https://pkg.jenkins.io/redhat-stable/jenkins.repo
    2  rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io-2023.key
    3  yum upgrade
    4  yum install java-17-amazon-corretto -y
    5  yum install jenkins -y
    6  systemctl enable jenkins
    7  systemctl start jenkins
    8  systemctl status jenkins
go to ithe browser and copy instance ip-address:8080 and create webhook
after sign in install plugins like maven , deploy to container , github integration then restart the jenkins
yum install maven
CONNECT TO APACHE SERVER
sudo su -
hostname = apache.example.com
bash
yum install java*
from google copy the link for tomcat 9 download select tar.gz file
 yum install wget
 wget https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.106/bin/apache-tomcat-9.0.106.tar.gz
 tar -xzf apache-tomcat-9.0.106.tar.gz
 ll
move to jenkins server 
mvn -v (copy the maven home path and java path and paste in the jenkins web server under tools)
next create new item and start the build if not coming install git on jenkins server
next move to apache server 
 cd  apache-tomcat-9.0.106
 cd bin
  ll
  chmod +x startup.sh
  chmod =x shutdown.sh
  chmod +x shutdown.sh
open the medium and follow the steps
after runnig the ./startup copy the ip address with port no 8080
login manager app username and pass admin 
then the configuration part and create credentials with username depolyer and pass depolyer
add container
make changes from github and check wheather the changes are reflecting or not
####################################################################################################
Docker_installation & Containers
:: yum install docker*

:: docker pull jenkins/jenkins

:: docker ps

:: systemctl start docker

:: systemctl enable docker

:: docker info

:: docker ps

:: Creating a container for jenkins
-- docker run -itd --name <Name_of_container> -p 8080:8080 jenkins/jenkins

:: docker ps -a

:: docker exec -it <container_id> /bin/bash

:: after we logged into 
-- <public ip-address>:8080
-- copy the directoryy

:: cat <paste_directory>

:: copy the password and paste it in browser login page
###################################################################################################
jenkins_on_aws
:: up to date on your instance by using
--- yum update –y

:: Add the Jenkins repo using the following command
--- wget -O /etc/yum.repos.d/jenkins.repo \
    https://pkg.jenkins.io/redhat-stable/jenkins.repo

:: Import a key file from Jenkins-CI to enable installation from the package
--- rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io-2023.key
--- yum upgrade

:: Install Java
--- yum install java-17-amazon-corretto -y

:: Install Jenkins
--- yum install jenkins -y

:: Enable the Jenkins service to start at boot
--- systemctl enable jenkins

:: Start Jenkins as a service
--- systemctl start jenkins


:: You can check the status of the Jenkins service using the command
--- systemctl status jenkins
 ............
 ###################3
 ################
 #################
 ###########
 :: yum install docker*

:: docker pull jenkins/jenkins

:: docker ps

:: systemctl start docker

:: systemctl enable docker

:: docker info

:: docker ps

:: Creating a container for jenkins
-- docker run -itd --name <Name_of_container> -p 8080:8080 jenkins/jenkins

:: docker ps -a

:: docker exec -it <container_id> /bin/bash

:: after we logged into 
-- <public ip-address>:8080
-- copy the directoryy

:: cat <paste_directory>

:: copy the password and paste it in browser login page
 ###########
 #########33
 ##########3
 ##########
 :: up to date on your instance by using
--- yum update –y

:: Add the Jenkins repo using the following command
--- wget -O /etc/yum.repos.d/jenkins.repo \
    https://pkg.jenkins.io/redhat-stable/jenkins.repo

:: Import a key file from Jenkins-CI to enable installation from the package
--- rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io-2023.key
--- yum upgrade

:: Install Java
--- yum install java-17-amazon-corretto -y

:: Install Jenkins
--- yum install jenkins -y

:: Enable the Jenkins service to start at boot
--- systemctl enable jenkins

:: Start Jenkins as a service
--- systemctl start jenkins


:: You can check the status of the Jenkins service using the command
--- systemctl status jenkins



 ############3
 ##########
 ##########33
 #########



 git 

 git ::
-------------
Server:

    1  yum install git -y

    2  ip a s

    3  hostname

    4  vim /etc/hosts

    5  cat vim /etc/hosts

    6  ping client-one.example.com

    7  ping client-two.example.com

    8  mkdir project

    9  cd project

   10  ls

   11  ls -a

   12  git init --bare

   13  ls -a

   14  cd

   15  passwd root

   16  vim /etc/ssh/sshd_config

   17  systemctl restart sshd

   18  systemctl enable sshd

   19  hostname

   20  cd project

   21  cd branches

   22  ls

   23  cd

   24  cd project

   25  ls -a

   26  cd branches

   27  ls -a

   28  cd ..

   29  ll

   30  ls -a

   31  cd branches/

   32  ls

   33  cd
 
----------------------------------------------------------------------------------------------------------------
 
Client-1:

    1  yum install git -y

    2  ip a s

    3  hostname

    4  vim /etc/hosts

    5  cat vim/etc/hosts

    6  cat /etc/hosts

    7  ping git-server.example.com

    8  ping client-two.example.com

    9  mkdir git-one

   10  cd git-one

   11  git init

   12  ls -a

   13  cd

   14  ssh-keygen

   15  cd .ssh/

   16  ls -a

   17  ssh-copy-id root@git-server.example.com

   18  cd

   19  cd git-one

   20  ll

   21  ls -a

   22  vim index.html

   23  git add index.html

   24  git commit -m "first" index.html

   25  git status

   26  git remote add origin root@git-server.example.com:/project

   27  git remote remove origin

   28  git remote add origin root@git-server.example.com:/project

   29  git push origin master

   30  git remote remove origin

   31  git remote add origin root@git-server.example.com:project

   32  git push origin master
 
------------------------------------------------------------------------------------------------------------------

Client-2:

    1  cd git-two

    2  git remote add origin root@git-server.example.com:project

    3  git pull origin master

    4  cat index.html



 ###################################################################################################################################################
 EFS (FOR CREATING A FILE IN AWS AND TRYING  TO GET IN REDHAT AND UBUNTU IN DIFFERENT 1A,1B,1C)

AWS
  1  yum install nfs-utils -y
    2  yum install
    3  yum install nfs-utils -y
    4  systemctl restart nfs
    5  systemctl start nfs
    6  systemctl start nfs-server.service
    7  systemctl enable nfs-server.service
    8  systemctl start nfs-server.service
    9  systemctl enable nfs-server.service
   10  systemctl status nfs-server.service
   11  mkdir efs
   12  sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport 172.31.34.186:/ efs
   13  df -h
   14  cd /root/efs
   15  touch devops{1..100}
   16  ll
   17  history
   18  touch cloud{1..10}
   19  ll
   20  mkdir uday
   21  ls
   22  ]cd
   23  cd
   24  cd efs
   25  mkdir yhk
   26  ls
   27  history

 REDHAT
  1  yum install nfs-utils -y
    2  systemctl start nfs-server.service
    3  systemctl enable nfs-server.service
    4  mkdir remote
    5  sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport 172.31.34.186:/ efs
    6  sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport 172.31.34.186:/ remote
    7  df -h
    8  cd /root/remote
    9  ls
   10  history
   11  ll
   12  ls
   13  cd
   14  ls
   15  cd r
   16  cd remote/
   17  ls
   18  history

 UBUNTU
     1  cat /etc/os-release
    2  apt-get update -y
    3  apt install nfs-common
    4  systemctl status nfs-
    5  syustem start nfs-common.service
    6  system start nfs-common.service
    7  system unmask nfs-common.service
    8  system start nfs-common.service
    9  system enable nfs-common.service
   10  systemctl status nfs-
   11  systemctl status nfs-common.service
   12  system enable nfs-common.service
   13  system start nfs-common.service
   14  system enable nfs-common.service
   15  history
   16  mkdir devops
   17  sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport 172.31.34.186:/ efs
   18  sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport 172.31.34.186:/ devops
   19  cd devops
   20  ls
   21  df -h
   22  ll
   23  history
   24  ll
   25  history

 TRYING TO CREATE IN 1C AN STOPPING 
 (TRAIL :
  1  mkdir srija
    2  sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport 172.31.34.186:/ srija
    3  ll
    4  cd /srija
    5  cd srija
    6  ll
    7  cd srija
    8  ll
    9  ls
   10  rmdir uday
   11  cd
   12  history)
 #############################################################################################################################
  yum update -y
    2  yum install automake fuse fuse-devel gcc-c++ git libcurl-devel libxml2-devel make openssl-devel
    3  git clone https://github.com/s3fs-fuse/s3fs-fuse.git
    4  ll
    5  curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
    6  unzip awscliv2.zip
    7  sudo ./aws/install
    8  aws --version
    9  aws configure
   10  ll
   11  cd .aws/
   12  ls
   13  cat config
   14  cat credentials
   15    cd
   16  sudo apt-get install automake autotools-dev fuse g++ git libcurl4-gnutls-dev libfuse-dev libssl-dev libxml2-dev make pkg-config
   17  apt-get install automake autotools-dev fuse g++ git libcurl4-gnutls-dev libfuse-dev libssl-dev libxml2-dev make pkg-config
   18  yum install automake fuse fuse-devel gcc-c++ git libcurl-devel libxml2-devel make openssl-devel
   19  git clone https://github.com/s3fs-fuse/s3fs-fuse.git
   20  ll
   21  /autogen.sh
   22  cd s3fs-fuse
   23  /autogen.sh
   24  cd s3fs-fuse/
   25  ./autogen.sh
   26  ./configure --prefix=/usr --with-openssl
   27  make
   28  make install
   29  which s3fs
   30  touch /etc/passwd-s3fs
   31  vim /etc/passwd-s3fs
   32  chmod 640 /etc/passwd-s3fs
   33  cd
   34  s3fs sriamju1234 /mnt -o passwd_file=/etc/passwd-s3fs
   35  df -h
   36  cd /mnt
   37  ll
   38  touch dev.txt{1..5}
   39  history
##########################################################################################################################
  ##########################################################################################################################
 creating own vpc

1 :-vpc :-  vpc only - name(devops-vpc) -ip(10.0.0.0/16) -create
2 :- internet gate way :- internet gateway - default(name it - default-iwg) -  create(devops-igw) - actions - attach to vpc
3 :-subnet - create - name(public-subnet) - zone(us-eat 1a) - (10.0.0.0/16) -(10.0.0.0/24)
             create - name(private-subnet) - zone(us-eat 1b) - (10.0.0.0/16) -(10.0.1.0/25)
4 :- route table :- route table - create - name(public-rt) - edit routs - 0.0000/00 - internet... 
                                                                                    - select yours...
                    click on public-rt - actions -edit subnet associations - select public
 5 :- instances :- ec2 - create - name(public-inst) - auto assign(enable) - vpc(select urs) - subnet (us east 1a) - security group (create new)
                   allow access to (http , icmp) -launch
                   ec2 - create - name(private-inst) - auto assign(disable) - vpc(select urs) - subnet (us east 1b) - security group (create new)
                   allow access to ( icmp) -launch

 
 open terminal of public-instance 
:- sudo su -
    yum install httpd -y 
    cd /var/www/html/
    echo "this is web server" >> index.html
    systemctl restart httpd
    systemctl enable httpd
    cd 
    ping google.com
   
 
 6 :- NAT :- crete - name(devops-nat) - subnet(public) -click allocate and create 
 7 :- route table :- route table - create - name(private-rt) - edit routs - 0.0000/00 - NAT... 
                                                                                    - select yours...
                    click on private-rt - actions - edit subnet associations - select private
      
 in terminal :-
     vim lti-mahape-key.pem  - now open the key in the system and paste the key 
    chmod 400 lti-mahape-key.pem
    ssh -i lti-mahape-key.pem ( open private-instance - connect -ssh  - in the url ther is (ec2-user@..) copy and pastee)
    ping google .com

 #########################################################################################################33
 #######################################################################################################

 To mount bucket and files 

 1 :- iam - users -create - name(sriii) - permissions - attach policy(amazons3full) 
      click on user - security credential - cli - tick on i understand - create - download csv file - done
  connect  to terminal 
      sudo su -
      1  yum update -y
    2  yum install automake fuse fuse-devel gcc-c++ git libcurl-devel libxml2-devel make openssl-devel
    3  git clone https://github.com/s3fs-fuse/s3fs-fuse.git
    4  ll
    5  curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
    6  unzip awscliv2.zip
    7  sudo ./aws/install
    8  aws --version
    9  aws configure
   10  ll
   11  cd .aws/
   12  ls
   13  cat config        give access key , secret key , tab
   14  cat credentials
   15    cd
   18  yum install automake fuse fuse-devel gcc-c++ git libcurl-devel libxml2-devel make openssl-devel
   19  git clone https://github.com/s3fs-fuse/s3fs-fuse.git
   20  ll
   21
   24  cd s3fs-fuse/
   25  ./autogen.sh
   26  ./configure --prefix=/usr --with-openssl
   27  make
   28  make install
   29  which s3fs
   30  touch /etc/passwd-s3fs
   31  vim /etc/passwd-s3fs    give(access key : secret key)
   32  chmod 640 /etc/passwd-s3fs
   33  cd
   34  s3fs name_of_the_bucket_where_files_are_present /mnt -o passwd_file=/etc/passwd-s3fs
   35  df -h
   36  cd /mnt
   37  ll
   38  touch dev.txt{1..5}
 everything will be mounted
 
#############################################################################################################################
 ###########################################################################################################################

 to make elastic ip
 1:-  instance - create - allow (http ) - security group (new) - storage(10 gib)- deletion on termination(yes)
 2:- open terminal
    # sudo su -
     yum install httpd -y
     rpmquery httpd
     cd /var/www/httpd
     cat > index.html
     systemctl start httpd 
     systemctl enable httpd
     systemctl status httpd 
 3:- copy the ip address and paste it will show the content 
 4:- reboot instance - and copy the ip address and paste it will show the content(ip will be changet)
 5:- elastic ip - create - click on allocate
     click on eip - actions - associate eip to instance - giver instance and private ip - click assosiate
 ###################################################################################################################################
 #################################################################################################################################

 To prevent instancee from termination and stopping
 1:-  instance - create(allow http)
  2:- open terminal
    # sudo su -
     yum install httpd -y
     rpmquery httpd
     cd /var/www/httpd
     cat > index.html
     systemctl start httpd 
     systemctl enable httpd
     systemctl status httpd 
 3:- click on the instance - actions - instance settings - click on stop protection / termination protection

 ###########################################################################################################################
 ##########################################################################################################################
 to make things happen on its own
 1:- instance - allow(http) -advance - type 
 #!/bin/bash
 yum install httpd -y
 echo "this is derver" >> /var/www/html/
 systemctl start httpd
 systemctl enable httpd
 useradd srija -p "redhat" -c "comment"

 2:- copy the ip and paste in the url
 3:-open terminal
 sudo su -
 cd /var/www/html
 cat index.html  (file is present with content )
 tail /etc/passwd
 tail /etc/shadow

 ############################################################################################################################3
 ##########################################################################################################################

 to create own templet
 1:-create templet - storage(ebs 10)
 2:- click on templet - actions - create instance from templet - chooser source - choose number of instances and launch 

 #########################################################################################################################
 ########################################################################################################################

[n.virginia] 1:-instance - create
             2:-volume - name existing one - click on that and click on create new volume - name - size (5) -save 
                click on new voulume - action s- attach - give instance and device - save
             3:- open terminal :- sudo su -
                                    mkdir /data
    2  lsblk
    3  mount /dev/xvdb /data
    4  mkfs.ext4 /dev/sdb
    5  mount /dev/xvdb /data
    6  cd /data
    7  touch dev.txt{1..5}
    8  ll
    9  cd
            4:- snapshots - create - volume - select new data volume 
                click on snapshot - action - copy - destnination -us-east-2 - copy snapshot
 [ohio]  - 1:- create new instance 
            2:- snapshot - click on snapshot -action - create new volume from shap shot
               - click on new volume - action -attch to instance - ...
           3:- open terminal
             sudo su -
                lsblk
             mkdir /data
              mount /dev/xvdb /data
             cd /data
             ll
               cd


 ############################################################################################################################
 #######################################################################################################################3
  EFS
 1:- instance - first - aws - us east 1a
                second - redhat -us east 1b
                third - ubuntu -us east 1a - enable 2049 nfs
 2 :- efs - create 
 3:- connect 
     terminal one -
      sudo su -
     yum install nfs-utils -y
    2  systemctl restart nfs-server.sevice
    3  systemctl enable nfs-server.service
     terminal two 
     sudo su -
      sytemctl start nfs-server.service
    2  systemctl enable nfs-server.service
     terminal three
     sudo su -
      systemctl status nfs-common.service
    3  systemctl start nfs-common.service
    4  systemctl unmask nfs-common.service
    5  systemctl start nfs-common.service
    6  systemctl enable nfs-common.service
    7  systemctl status nfs-common.service

 4:- security group - create(efs-sg) - desc - invound(add rule nfs set value ) - create
 5:- efs -filesystem -network mange - select efs-sg for 1a and 1b) - create
     attach - mount ip -copy the url 
 6:- terminal
     one -mkdir efs
    5  sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport 172.31.41.201:/ efs
    6  df -h
    7  cd /root/efs
    8  touch dev.txt{1..5}
    9  ll
   10  cd
    two 
      mkdir remote
    4  sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport 172.31.41.201:/ remote
    5  df -h
    6  cd /remote
    7  cd remote
    8  ll
    9  cd
   three
     mkdir rem
   10  sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport 172.31.41.201:/ rem
   11  cd rem
   12  ll
 #############################################################33333333####################################################
 #######################################################################################################################

 Tomcat , jenkins and github 
 1:- instance(dev-server) - 
 2:-  connect the instance 
    in terminal :-  
     sudo su -
     ssh-keygen
   
    3  cd .ssh/
    4  ll
    6  cat id_rsa.pub
    7  yum install git -y
    8  cd
   3:-  open github - create a new repo 
     settings - ssh key - new ssh -paste the content of the key
   4:- git clone https://github.com/sanjayguruji/web-repo.git
   
   19  cd web-repo
   20  git init
   21  git add .
   22  git commit -m "first commit"
   23  git branch -M main
   24  git remote -v
   25  git remote remove origin
   26  git remote add origin git@github.com:AMJURI329/new.git
   27  git remote -v
   28  git push -u origin main
   29  cd
 5:- check in the github whether u got the details or not 
 6:- create a jenkins instance - medium - storage(15)
     connect terminal -
      sudo su -
       yum update –y
    2  wget -O /etc/yum.repos.d/jenkins.repo     https://pkg.jenkins.io/redhat-stable/jenkins.repo
    3  rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io-2023.key
    4  yum upgrade
    5  yum install java-17-amazon-corretto -y
    6  yum install jenkins -y
    7  systemctl enable jenkins
    8  systemctl start jenkins
    9  systemctl status jenkins
    7:- now copy ip address of jenkins instance and paste:8080
    copy the url and in terminal : cat ...url...
    paste the passwd
    install plugins- username ...details...
    8:  github - settings -webhooks - new webhook -ipaddres of jenkins(/github-webhook/)- json -secret(jenkin - profile -security -add new token -)
     9:- jenkin dashboard manage jenkins - plugin - additional plugin -maven ,deploy to coontainer ,github integration install 
     in terminal jenkins -yum install maven 
                          yum install git -y
                          mvn -v (copy this)
    
    10:- jenkins  - new item -maven -create - git - */main
          jenkins - tools - java - maven 
         
   11:- create apache instance -medium - connect - 
        in terminal - sudo su -hostnamectl set-hostname tomcat.com - bash - yum install java* -  yum install wget -  wget https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.105/bin/apache-tomcat-9.0.105.tar.gz -
           tar -xvzf apache-tomcat-9.0.105.tar.gz - ll - cd  apache-tomcat-9.0.105 - ll -  chmod +x startup.sh - chmod +x shutdown.sh - cd .. - cd conf/ - ll - vim tomcat-users.xml - cd .. - vi ./webapps/examples/META-INF/context.xml - vi ./webapps/host-manager/META-INF/context.xml -
           vi ./webapps/manager/META-INF/context.xml - ll - cd bin/ - ./startup.sh
   12 - now open apache - copy ip of tomcat : 8080 - managet app - admin admin 
   13:- in jenkins - configure click postbuild - deploy war / ear - war (**/*.war) - content path(/) - add container (tomcat 9) - 
      url(tomcact url 8080) -credentials selcect 
     14: manage jenkns - credential - global - deployer deployer 
   for auromation :- configure - github hook trigger 
 ################################################################################################################################
 ##################################################################################################################################

 ******Using 2 vpc sending data from one region to another*******
N.Virgina
Create vpc :- dev-vpc , 10.0.0.0/16
Igw :- dev-igw , attach igw to vpc
subnet :- public-subnet , 1a , 10.0.0.0/16 , 10.0.0.0/24
          private-subnet , 1b , 10.0.0.0/16, 10.0.1.0/24
route table :- public-rt , dev-vpc
edit routes :- 0.0.0-- , igw , save changes
route table :- private-rt , dev-vpc 
associate subnets
create instance :- public-server , lti keuy , dev-vpc , public-subnet , create sg , add http , icmp
create instance :- private-server , dev-vpc , private-subnet , disabled , existing sg
connect 
#sudo su -
#yum install httpd -y
#cd /var/www/html
#echo "this is web server" > index.html
#ll
#systemctl restart httpd
#systemctl enable httpd
paste public ip of ng instance
#cd
copy private-server ip add
#ping ip_add
----------ohio-region----------
create vpc :- AI-project , 20.0.0.0/16
igw - AI-project-igw , attach to vpc 
subnet :- web-subnet , 2a , 20.0.0.0/24
          db-subnet , 2b, 20.0.1.0/24
route table :- web-rt , AI-project,   
edit routes :- igw
associate subnets
route table :- db-rt 
instances :- create :- web-server , sg(web-sg)(new) , allow icmp , http
create : db-server 
connect
#sudo su -
#yum install httpd -y
#ping ip_add (private ip of db_Server)
#passwd root 
#vim /etc/ssh/sshd_config (edit permintrootpassword , passwordauthentication -> yes)
systemctl restart sshd
systemctl enable sshd
do these changes in the both machines
create files on the both the machines 
cat > devops.txt provide some content in that (similary do it in N.virgina region)
create peering connections 
name : dev-to-ai
vpc : dev-pc
another region 
us-east-2
vcpid (copy vpc id from the ohio region)
in ohio accept request -> under peering connections 
--------ohio-region----------------
route table :- web-rt 
edit :- add (10.0.0.0/16)  peering connection
--------N.V------------
route table :- private
edit :- add(20.0.0.0/16) peering connection 
------ohio terminal----------
scp devops.txt root@public-ip-(nv-region-termial):/tmp
------nv terminal----------
cd /tmp
ll
vice-versa (optional)
=====================================================================================================
MFA 
first login to aws account -> security credentials -> mfa device
give mfa device name -> scane using authentictor app -> provide two codes that comes in authenticator app 
you can also disable it uh will see a option to remove 
========================

 #######################################################################################################################################################################
 ###################################################################################################################################################################

 DOCKER

 launch instance - allow ports on which containers are going to run (i.e, 8080,8081,8082)
 #yum install docker*
   systemctl enable docker
  systemctl start docker
  docker info 
 docker ps -a
 docker image ls
 docker --help
 docker pull ubuntu
 docker pull httpd 
 docker run -it --name web-app -p 8080:8080 ubuntu /bin/bash
    apt update -y
    apt install apache2 -y
    cd /var/www/html
    echo "this is web" > index.html
    service apache2 start
    (ctrl p ctrl q ) to come out
 curl http://(ip of the instance)
 docker inspect web-app | less     #(get ip from here)
 
  docker run -it --name web-app1 -p 8081:8080 ubuntu /bin/bash
    apt update -y
    apt install apache2 -y
    cd /var/www/html
    echo "this is web1" > index.html
    service apache2 start
    (ctrl p ctrl q ) to come out
 curl http://(ip of the instance)
 
 docker stop web-app
 docker rm web-app

 docker kill web-app
 docker rm web-app


 creating volume 
  docker volume create my-vol
   docker volume ls
   docker run -it -v my-vol:/var/www/html -p 8083:80 --name vol1 ubuntu /bin/bash
      apt update -y
     apt install apache2 -y  
    cd /var/www/html
       echo "this is vol" > index.html
       cd
     service apache2 start
     ctrlp + ctrl Q
      cd /var/lib/docker/volumes
      ll
       docker stop vol1
     docker rm vol1
     docker run -it -v my-vol:/var/www/html --name vol2 ubuntu /bin/bash
     cd /var/www/html
      ll
     cat index.html


 files ---------
  mkdir /data
  docker run -it -v /data/:/tmp --name copying ubuntu /bin/bash
  cd /tmp
   touch data1.txt{1..5}
   cd
   ctrlP+ctrlq
   cd /data
   ll
 (all files will be visible )
      
 
 


 ###############################################################################################################################################################
 #########################################################################################################################################################################

 TERRAFORM
----------------------------------------------------------------------------------------------=----------------------------------------------------------------------------
 this is for adding the inbound and outbound rules
 vim server.tf
 provider "aws" {
  region     = "us-east-1"
  access_key = ""
  secret_key = ""
}
 resource "aws_instance" "web-server" {
  ami                    = "ami-05ffe3c48a9991133"
  instance_type          = "t2.micro"
  key_name               = "lti-mahape-key"
  availability_zone      = "us-east-1a"
  security_groups = ["${aws_security_group.web-access.name}"]
  tags = {
    Name = "hello"
  }
}

resource "aws_security_group" "web-access" {
  name        = "web-access"
  description = "Allow access"
  tags = {
    Name = "web-access"
  }
}

resource "aws_vpc_security_group_ingress_rule" "allow_http" {
  security_group_id = aws_security_group.web-access.id
  cidr_ipv4         = "0.0.0.0/0"
  from_port         = 80
  ip_protocol       = "tcp"
  to_port           = 80
}
resource "aws_vpc_security_group_ingress_rule" "allow_ssh" {
  security_group_id = aws_security_group.web-access.id
  cidr_ipv4         = "0.0.0.0/0"
  from_port         = 22
  ip_protocol       = "tcp"
  to_port           = 22
}
resource "aws_vpc_security_group_ingress_rule" "allow_443" {
  security_group_id = aws_security_group.web-access.id
  cidr_ipv4         = "0.0.0.0/0"
  from_port         = 443
  ip_protocol       = "tcp"
  to_port           = 443
}
resource "aws_vpc_security_group_egress_rule" "allow_all_traffic_ipv4" {
  security_group_id = aws_security_group.web-access.id
  cidr_ipv4         = "0.0.0.0/0"
  ip_protocol       = "-1"
}



---------------------------------------------------------------------------------------------------------------------------------------------------------------------------


This is related to the attaching the volume to the instance

 provider "aws" {
  region     = "us-east-1"
  access_key = ""
  secret_key = ""
}
resource "aws_security_group" "web-access3" {
  name        = "web-access3"
  description = "Allow access"
  tags = {
    Name = "web-access3"
  }
}

resource "aws_vpc_security_group_ingress_rule" "allow_http1" {
  security_group_id = aws_security_group.web-access3.id
  cidr_ipv4         = "0.0.0.0/0"
  from_port         = 80
  ip_protocol       = "tcp"
  to_port           = 80
}
resource "aws_instance" "creating_vol" {
  ami               = "ami-05ffe3c48a9991133"
  availability_zone = "us-east-1a"
  instance_type     = "t2.micro"
  security_groups   = ["${aws_security_group.web-access3.name}"]
  key_name          = "lti-mahape-key"
  #root disk
  root_block_device {
    volume_size           = "25"
    volume_type           = "gp2"
    delete_on_termination = true
  }
 
  #additional data disk
  ebs_block_device {
    device_name           = "/dev/xvdb"
    volume_size           = "10"
    volume_type           = "gp2"
    delete_on_termination = true
  }

  user_data = <<-EOF
        #!/bin/bash
        sudo yum install httpd -y
        sudo systemctl start httpd
        sudo systemctl enable httpd
        echo "<h1>sample webserver using terraform</h1>" | sudo tee /var/www/html/index.html
  EOF

  tags = {
    Name     = "hello-India"
    Stage    = "testing"
    Location = "India"
  }

}

vim lti-mahape-key.pem  -  paste the key of the lti-mahape-key.pem
chmod 400 lti-mahape-key.pem
   paste the connect ssh to connect the new instance 
   sudo su -
   lsblk
   mkfs.ext4 /dev/xvdb
   mkdir /data
   mount /dev/xvdb /data
   cd /data
   touch sri.txt{1..5}
   ll
------------------------------------------------------------------------------------------------------------------------------------------------------------------------
   


for using the key paie that is created for security 
   provider "aws" {
  region     = "us-east-1"
  access_key = ""
  secret_key = ""
}

resource "aws_instance" "creating_vol" {
  ami               = "ami-05ffe3c48a9991133"
  availability_zone = "us-east-1a"
  instance_type     = "t2.micro"

  key_name = "deployer-key"
  #root disk
  root_block_device {
    volume_size           = "25"
    volume_type           = "gp2"
    delete_on_termination = true
  }

  #additional data disk
  ebs_block_device {
    device_name           = "/dev/xvdb"
    volume_size           = "10"
    volume_type           = "gp2"
    delete_on_termination = true
  }
   
  user_data = <<-EOF
        #!/bin/bash
        sudo yum install httpd -y
        sudo systemctl start httpd
        sudo systemctl enable httpd
        echo "<h1>sample webserver using terraform</h1>" | sudo tee /var/www/html/index.html
  EOF

  tags = {
    Name     = "hello-India"
    Stage    = "testing"
    Location = "India"
  }

}

resource "aws_key_pair" "deployer" {
  key_name   = "deployer-key"
  public_key = ""
}   



for using existing security group 
 provider "aws" {
  region     = "us-east-1"
  access_key = ""
  secret_key = ""
}
resource "aws_instance" "creating_vol" {
  ami                    = "ami-05ffe3c48a9991133"
  availability_zone      = "us-east-1a"
  instance_type          = "t2.micro"
  vpc_security_group_ids = [data.aws_security_group.previous-sg.id]
  key_name               = "lti-mahape-key"
  tags = {
    Name     = "existing_sg_inst"
    Stage    = "testing"
    Location = "India"
  }
}

data "aws_security_group" "previous-sg" {
  id = "sg-0bf9399b22dafa7fc" #existing security group id
}




this is for provisioning automatically
   provider "aws" {
  region     = "us-east-1"
  access_key = ""
  secret_key = ""
}

data "aws_security_group" "previous-sg" {
  id = "sg-0bf9399b22dafa7fc" #existing security group id
}
data "aws_ami" "ubuntu" {
  most_recent = true

  filter {
    name   = "name"
    values = ["ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*"]
  }

  filter {
    name   = "virtualization-type"
    values = ["hvm"]
  }

  owners = ["099720109477"] # Canonical
}

resource "aws_instance" "web" {
  ami                    = data.aws_ami.ubuntu.id
  instance_type          = "t2.micro"
  availability_zone      = "us-east-1a"
  key_name               = "lti-mahape-key"
  vpc_security_group_ids = [data.aws_security_group.previous-sg.id]
  tags = {
    Name = "HelloWorld"
  }
}

   
 
mkdir project
    2  cd project
    3  vim server.tf
    4  terraform init
    5  cd
    6  sudo dnf install -y dnf-plugins-core
    7  sudo dnf config-manager --add-repo https://rpm.releases.hashicorp.com/AmazonLinux/hashicorp.repo
    8  sudo dnf -y install terraform
    9  curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
   10  unzip awscliv2.zip
   11  sudo ./aws/install
   12  terraform --version
   13  cd project
   14  ll
   15  terraform init
   16  vim server.tf
   17  terraform fmt
   18  terraform validate
   19  terraform plan
   20  terraform apply
 21

------------------------------------------------------------------------------------------------------------------------------------------------------------------------
   for vpc creation 
 provider "aws" {
  region     = "us-east-1"
  access_key = ""
  secret_key = ""
}

resource "aws_vpc" "test-vpc" {
  cidr_block = "10.0.0.0/16"
}
resource "aws_subnet" "public-subnet" {
  vpc_id            = aws_vpc.test-vpc.id
  availability_zone = "us-east-1a"
  cidr_block        = "10.0.0.0/24"

  tags = {
    Name = "Public-subnet"
  }
}
resource "aws_subnet" "private-subnet" {
  vpc_id            = aws_vpc.test-vpc.id
  availability_zone = "us-east-1b"
  cidr_block        = "10.0.1.0/24"

  tags = {
    Name = "Private-subnet" #security group
  }
}
resource "aws_security_group" "test_access" {
  name        = "test_access"
  vpc_id      = aws_vpc.test-vpc.id
  description = "allow ssh and http"

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }


}
resource "aws_internet_gateway" "test-igw" {
  vpc_id = aws_vpc.test-vpc.id

  tags = {
    Name = "test-igw"
  }
}

resource "aws_route_table" "public-rt" {
  vpc_id = aws_vpc.test-vpc.id

  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.test-igw.id
  }


  tags = {
    Name = "public-rt"

  }
}
resource "aws_route_table_association" "public-asso" {
  subnet_id      = aws_subnet.public-subnet.id
  route_table_id = aws_route_table.public-rt.id
}
resource "aws_instance" "sri-server" {
  ami             = "ami-05ffe3c48a9991133"
  subnet_id       = aws_subnet.public-subnet.id
  instance_type   = "t2.micro"
  security_groups = ["${aws_security_group.test_access.id}"]
  key_name        = "ltimindtree"
  tags = {
    Name     = "test-World"
    Stage    = "testing"
    Location = "chennai"
  }

}
resource "aws_eip" "sri-ec2-eip" {
  instance = aws_instance.sri-server.id
}
resource "aws_key_pair" "ltimindtree" {
  key_name   = "ltimindtree"
  public_key = "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDsiVHsGsczvFyiEezuKaVb60WPaWJ7l+9HiE6qMOAVY0qftZZx8CRRtnwPsHt6bSBTTfM5P5e4nKZF7AWG+qU2Mr1zjVOlHJHSJOi9StJrqEOwZMj/xkzZ1KDuFahpFHYTDYUR/kjPiZfOEyrsuaCVB6KwyidnseJjG/GJwrLOCSro2/dEfGWPS8k/b/UAUiarGpavD+kWqCQfNOfrfAiLkQztaUZiT7uHUQ9TmmEUXqNliP0B7pnXEHodi/E3xn91/3nWVVvklyCYfpx7r2XEteEq7imGrP5U9Y4G5VB7o/L9ZDcjXDuoCkEtnMGOuXwJgp5n3PaIippiM7sPWl79q4GcRVg+7j+aET1J5Edda6e99g9ukjLcQts7+rESQEyzp28T6opcmF5wvvhLP0r1DwwFCv6LXX1K3QDrmYr48M+gWIeCi7MWePZfeP5u+jFjGTKoHvgQrt0Rpm+VixgYM91WTqiWjklFs7ArVGczR5+XZcCz1/6AeszPzF7Cwr8= root@ip-172-31-40-165.ec2.internal"
}
resource "aws_instance" "database-server" {
  ami             = "ami-05ffe3c48a9991133"
  subnet_id       = aws_subnet.private-subnet.id
  instance_type   = "t2.micro"
  security_groups = ["${aws_security_group.test_access.id}"]
  key_name        = "ltimindtree"
  tags = {
    Name     = "db-World"
    Stage    = "stage-base"
    Location = "delhi"
  }

}
resource "aws_eip" "nat-eip" {
  tags = {
    Name = "nat-eip"
  }
}
resource "aws_nat_gateway" "my-ngw" {
  allocation_id = aws_eip.nat-eip.id
  subnet_id     = aws_subnet.public-subnet.id
}

resource "aws_route_table" "private-rt" {
  vpc_id = aws_vpc.test-vpc.id

  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_nat_gateway.my-ngw.id
  }


  tags = {
    Name = "private-rt"
  }
}
resource "aws_route_table_association" "private-asso" {
  subnet_id      = aws_subnet.private-subnet.id
  route_table_id = aws_route_table.private-rt.id
}





    using variables

   provider "aws" {
  region     = "us-east-1"
  access_key = ""
  secret_key = ""
}
variable "vpc_cidr" {
  default     = "10.1.0.0/16"
  description = "cidr for our custom vpc"
}

variable "subnet_cidr" {
  default     = "10.1.1.0/24"
  description = "cidr for subnet"
}

variable "availability_zone" {
  default     = "us-east-1a"
  description = "AZ for subnet"
}

variable "instance_ami" {
  default     = "ami-05ffe3c48a9991133"
  description = "default ami for instances"
}

variable "instance_type" {
  default     = "t2.micro"
  description = "instance type for ec2"
}
   variable "env_tag" {
  default     = "production"
  description = "environment tag"
}


# code - creating vpc

resource "aws_vpc" "vpcone" {
  cidr_block = var.vpc_cidr
  tags = {
    Name = "${var.env_tag}"
  }
}

# code - creating IG and attaching it to VPC

resource "aws_internet_gateway" "vpcone-ig" {
  vpc_id = aws_vpc.vpcone.id
  tags = {
    Name = "${var.env_tag}"
  }
}
resource "aws_subnet" "subnet_public" {
  vpc_id                  = aws_vpc.vpcone.id
  cidr_block              = var.subnet_cidr
  map_public_ip_on_launch = "true"
  availability_zone       = var.availability_zone
  tags = {
    Name = "${var.env_tag}"
  }

}

# code - modifying route

resource "aws_route_table" "rtb_public" {
  vpc_id = aws_vpc.vpcone.id
  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.vpcone-ig.id
  }
  tags = {
    Name = "${var.env_tag}"
  }
}
resource "aws_route_table_association" "rta_subnet_public" {
  subnet_id      = aws_subnet.subnet_public.id
  route_table_id = aws_route_table.rtb_public.id
}
resource "aws_security_group" "sg_newvpc" {
  name   = "newvpc"
  vpc_id = aws_vpc.vpcone.id

  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "${var.env_tag}"
  }

}
   
resource "aws_instance" "test" {
  ami                    = var.instance_ami
  instance_type          = var.instance_type
  subnet_id              = aws_subnet.subnet_public.id
  vpc_security_group_ids = ["${aws_security_group.sg_newvpc.id}"]
  tags = {
    Name = "${var.env_tag}"
  }
}
          
       
 
 
 
 
                
    
 
                                      
